[
  {
    "id": 1,
    "question": "In Spark, repartition function can :",
    "choices": [
      "only increase the number of partitions",
      "only decrease the number of partitions",
      "increase or decrease the number of partitions",
      "automatically repartition the data"
    ],
    "correct": 2,
    "description": "Spark provides two ways to control the number of partitions. One of them is repartition().\nRepartition can increase and decrease the number of partitions. It performs a full shuffle of data across partitions and equally distributes them resulting in roughly equal sized partitions.\nIt creates new partitions with data that's distributed evenly (data distribution is more even for larger data sets) and returns new RDD.\nIt is a very expensive operation as it shuffles data from all nodes in a cluster, so one must be careful when using it.\nWhen you call the repartition() function, Spark internally calls the coalesce function with shuffle parameter set to true.\nDefault Spark hash partitioning function will be used to repartition the dataframe."
  },
  {
    "id": 2,
    "question": "Which of the following statements about the DAG is correct?",
    "choices": [
      "DAG stands for Distributed Acyclic Graph",
      "Spark developers cannot see the DAG of Spark jobs because Spark wants hide internals for simplicity",
      "A DAG represents the executor level mapping of tasks",
      "A DAG represents the execution plan of a Spark job"
    ],
    "correct": 3,
    "description": "DAG stands for Directed Acyclic Graph. It is the logical execution plan of a Spark job.\nHigh-level operations(i.e., transformations and actions) in the Spark application code is converted into a DAG of stages and tasks.\nJob is broken down into a sequence of stages, and each stage has a group of tasks that are executed independently across partitions.\nThe DAG allows Spark to perform various optimizations, such as pipelining, task reordering, and pruning unnecessary operations, to improve the efficiency of the job execution.\nDAG Scheduler is responsible for transforming a sequence of RDD operation into a DAG.\nFault tolerance for Spark is achieved using the DAG's lineage, which is the record of the transformations that were used to create an RDD."
  },
    {
    "id": 3,
    "question": "What is the main purpose of Apache Spark?",
    "choices": [
      "To store large datasets",
      "To process data faster than Hadoop MapReduce",
      "To replace databases",
      "To run machine learning models only"
    ],
    "correct": 1,
    "description": "Apache Spark is an open-source, distributed processing system used for big data workloads.\nIt provides high-level APIs in Java, Scala, Python, and R, and an optimized engine that supports general execution graphs.\nSpark is designed to be much faster than Hadoop MapReduce due to in-memory computing and DAG execution."
  },
  {
    "id": 4,
    "question": "Which of the following is NOT a component of Spark?",
    "choices": [
      "Spark Core",
      "Spark SQL",
      "Spark Streaming",
      "Spark Database"
    ],
    "correct": 3,
    "description": "Apache Spark has several core components:\n- Spark Core (RDDs, task scheduling)\n- Spark SQL (structured data)\n- Spark Streaming (real-time data)\n- MLlib (machine learning)\n- GraphX (graph processing)\nThere is no component called 'Spark Database'."
  },
  {
    "id": 5,
    "question": "What does RDD stand for?",
    "choices": [
      "Resilient Data Dataset",
      "Resilient Distributed Dataset",
      "Remote Data Distribution",
      "Reusable Data Definition"
    ],
    "correct": 1,
    "description": "RDD stands for **Resilient Distributed Dataset**.\nIt is the fundamental data structure of Spark.\nRDDs are immutable, distributed collections of objects that can be processed in parallel.\nThey are fault-tolerant and support in-memory computation."
  },
  {
    "id": 6,
    "question": "Which function is used to reduce the number of partitions without shuffling?",
    "choices": [
      "repartition()",
      "coalesce()",
      "shuffle()",
      "reduceByKey()"
    ],
    "correct": 1,
    "description": "The **coalesce()** function reduces the number of partitions without performing a full shuffle.\nIt is more efficient than repartition() when reducing partitions.\nUse coalesce() after filtering a large dataset to avoid unnecessary shuffles."
  },
  {
    "id": 7,
    "question": "What happens when you call collect() on an RDD?",
    "choices": [
      "It returns a new RDD",
      "It brings all data to the driver node",
      "It saves data to disk",
      "It triggers lazy evaluation"
    ],
    "correct": 1,
    "description": "**collect()** is an **action** that returns all elements of the RDD to the driver program as an array.\nUse it only on small datasets — large data can crash the driver.\nIt forces computation of all transformations in the lineage."
  },
  {
    "id": 8,
    "question": "Which of the following is a transformation?",
    "choices": [
      "count()",
      "collect()",
      "map()",
      "take()"
    ],
    "correct": 2,
    "description": "Transformations create a new RDD from an existing one and are **lazy** (not executed immediately).\nExamples: map(), filter(), flatMap(), groupByKey()\nActions trigger computation: count(), collect(), saveAsTextFile()\n**map()** is a transformation."
  },
  {
    "id": 9,
    "question": "What is the default partitioning strategy in Spark?",
    "choices": [
      "Range partitioning",
      "Hash partitioning",
      "Round-robin partitioning",
      "Custom partitioning"
    ],
    "correct": 1,
    "description": "By default, Spark uses **hash partitioning** when shuffling data (e.g., in groupByKey, reduceByKey, join).\nThe partition is determined by: hash(key) % numPartitions\nThis ensures keys with the same value go to the same partition."
  },
  {
    "id": 10,
    "question": "Which file format is columnar and supports predicate pushdown?",
    "choices": [
      "CSV",
      "JSON",
      "Parquet",
      "Text"
    ],
    "correct": 2,
    "description": "**Parquet** is a columnar storage file format optimized for complex data processing.\nIt supports:\n- Column pruning (read only needed columns)\n- Predicate pushdown (filter at storage level)\n- Efficient compression and encoding\nIdeal for analytical queries."
  },
  {
    "id": 11,
    "question": "What is lazy evaluation in Spark?",
    "choices": [
      "Spark executes code slowly",
      "Transformations are not computed until an action is called",
      "Spark delays writing to disk",
      "Actions are executed lazily"
    ],
    "correct": 1,
    "description": "Spark uses **lazy evaluation**: transformations (map, filter) build a logical plan (DAG) but do **not** execute immediately.\nOnly when an **action** (collect, count) is called does Spark compute the result.\nThis enables optimization and avoids unnecessary work."
  },
  {
    "id": 12,
    "question": "Which of the following is an action?",
    "choices": [
      "filter()",
      "map()",
      "reduceByKey()",
      "first()"
    ],
    "correct": 3,
    "description": "Actions return a value to the driver program after running a computation.\nExamples: collect(), count(), first(), take(), saveAsTextFile()\n**first()** returns the first element of the RDD — it is an action."
  },
  {
    "id": 13,
    "question": "What is the role of the Spark Driver?",
    "choices": [
      "Executes tasks on worker nodes",
      "Stores data in memory",
      "Manages the application and coordinates tasks",
      "Handles shuffle operations"
    ],
    "correct": 2,
    "description": "The **Spark Driver** is the central coordinator.\nIt:\n- Runs the main() function\n- Creates SparkContext\n- Converts user code into tasks\n- Schedules tasks on executors\n- Collects results\nIt does **not** execute tasks — that’s done by executors."
  },
  {
    "id": 14,
    "question": "Which method is used to cache an RDD in memory?",
    "choices": [
      "save()",
      "persist()",
      "collect()",
      "store()"
    ],
    "correct": 1,
    "description": "**persist()** or **cache()** stores an RDD in memory (or disk) for faster reuse.\ncache() is a shortcut for persist(MEMORY_ONLY)\nUseful for iterative algorithms or when an RDD is used multiple times.\nUnpersist() to remove from cache."
  },
  {
    "id": 15,
    "question": "What is a Spark Stage?",
    "choices": [
      "A single task",
      "A group of tasks that can be executed without shuffling",
      "The entire job",
      "A physical machine"
    ],
    "correct": 1,
    "description": "A **stage** is a set of tasks that can be executed in parallel without data shuffling.\nSpark splits a job into stages at **shuffle boundaries** (e.g., groupByKey, join).\nEach stage contains tasks — one per partition.\nStages are executed sequentially."
  },
  {
    "id": 16,
    "question": "Which of the following is true about DataFrame?",
    "choices": [
      "It is untyped and schema-less",
      "It is a collection of Rows with a schema",
      "It cannot be created from RDD",
      "It is slower than RDD"
    ],
    "correct": 1,
    "description": "A **DataFrame** is a distributed collection of data organized into named columns.\nIt has a **schema** (column names and types).\nBuilt on top of RDD but with optimizations via Catalyst optimizer.\nCan be created from RDD, JSON, CSV, Parquet, etc."
  },
  {
    "id": 17,
    "question": "What is the default shuffle partition count in Spark?",
    "choices": [
      "1",
      "100",
      "200",
      "1000"
    ],
    "correct": 2,
    "description": "The default value of **spark.sql.shuffle.partitions** is **200**.\nThis controls the number of partitions after a shuffle (e.g., join, groupBy).\nFor large datasets, you may need to increase it.\nFor small data, reduce to avoid overhead."
  }
]