[
  {
    "id": 1,
    "question": "What is Apache Spark primarily used for?",
    "choices": [
      "Large-scale data processing",
      "Web application hosting",
      "Mobile app development",
      "Image compression"
    ],
    "correct": "Large-scale data processing",
    "description": "Spark is a distributed computing framework designed to process large amounts of data quickly."
  },
  {
    "id": 2,
    "question": "Which organization originally developed Apache Spark?",
    "choices": [
      "Google",
      "Databricks",
      "UC Berkeley AMPLab",
      "Facebook"
    ],
    "correct": "UC Berkeley AMPLab",
    "description": "Spark was started at UC Berkeley’s AMPLab before being donated to Apache."
  },
  {
    "id": 3,
    "question": "Spark stores data in memory for computation to provide:",
    "choices": [
      "Lower storage costs",
      "Higher performance than disk-based engines",
      "Better integration with relational databases",
      "Improved GUI support"
    ],
    "correct": "Higher performance than disk-based engines",
    "description": "Spark uses in-memory computation which speeds up iterative workloads significantly."
  },
  {
    "id": 4,
    "question": "Spark is written in which programming language?",
    "choices": [
      "Java",
      "Scala",
      "Python",
      "C++"
    ],
    "correct": "Scala",
    "description": "Spark is written in Scala but supports multiple high-level languages like Python, Java, and R."
  },
  {
    "id": 5,
    "question": "Which of the following is NOT a core Spark component?",
    "choices": [
      "Spark SQL",
      "Spark Streaming",
      "Spark MLlib",
      "Hadoop YARN"
    ],
    "correct": "Hadoop YARN",
    "description": "YARN is a cluster manager, not part of Spark’s internal ecosystem."
  },
  {
    "id": 6,
    "question": "What type of framework is Spark?",
    "choices": [
      "Distributed data processing engine",
      "Single-threaded CPU engine",
      "Relational database",
      "Web application framework"
    ],
    "correct": "Distributed data processing engine",
    "description": "Spark performs distributed computation over clusters for big data workloads."
  },
  {
    "id": 7,
    "question": "Which feature makes Spark ideal for machine learning?",
    "choices": [
      "Streaming-only operations",
      "In-memory computation",
      "Built-in data visualization",
      "File indexing capability"
    ],
    "correct": "In-memory computation",
    "description": "ML requires iterative computations which benefit greatly from Spark's in-memory models."
  },
  {
    "id": 8,
    "question": "Which deployment mode allows Spark to run standalone without Hadoop?",
    "choices": [
      "YARN mode",
      "Standalone mode",
      "Mesos mode",
      "Docker mode"
    ],
    "correct": "Standalone mode",
    "description": "Standalone mode includes Spark’s built-in cluster manager."
  },
  {
    "id": 9,
    "question": "Spark uses which data abstraction for distributed datasets?",
    "choices": [
      "RDD",
      "DataFrame only",
      "SQL Tables only",
      "JSON Documents"
    ],
    "correct": "RDD",
    "description": "Resilient Distributed Datasets (RDDs) are the original core data abstraction in Spark."
  },
  {
    "id": 10,
    "question": "Which Spark component supports structured data and SQL queries?",
    "choices": [
      "GraphX",
      "Spark SQL",
      "MLlib",
      "Core RDD API"
    ],
    "correct": "Spark SQL",
    "description": "Spark SQL enables structured queries using SQL and DataFrames."
  },
  {
    "id": 11,
    "question": "GraphX is a Spark library for:",
    "choices": [
      "Graph processing",
      "Image transformation",
      "Unstructured text processing",
      "Cloud deployment"
    ],
    "correct": "Graph processing",
    "description": "GraphX lets you compute over graphs using vertices and edges."
  },
  {
    "id": 12,
    "question": "Spark Streaming processes data in:",
    "choices": [
      "Micro-batches",
      "Single record per second",
      "Only static batches",
      "Cloud-only environments"
    ],
    "correct": "Micro-batches",
    "description": "Spark Streaming uses DStreams that operate on small time-based batches."
  },
  {
    "id": 13,
    "question": "Which cluster managers does Spark support?",
    "choices": [
      "Only YARN",
      "Only Kubernetes",
      "Standalone, YARN, Mesos, Kubernetes",
      "None — Spark has no cluster support"
    ],
    "correct": "Standalone, YARN, Mesos, Kubernetes",
    "description": "Spark is highly flexible and integrates with multiple cluster managers."
  },
  {
    "id": 14,
    "question": "Spark applications are typically written in:",
    "choices": [
      "Scala, Python, Java, R",
      "Only C++",
      "Only Assembly",
      "JavaScript only"
    ],
    "correct": "Scala, Python, Java, R",
    "description": "These are officially supported high-level APIs."
  },
  {
    "id": 15,
    "question": "MLlib is a library in Spark for:",
    "choices": [
      "Machine learning",
      "Data ingestion",
      "Dashboard creation",
      "Distributed security"
    ],
    "correct": "Machine learning",
    "description": "It provides scalable ML algorithms including classification, clustering, etc."
  },
  {
    "id": 16,
    "question": "Spark can process:",
    "choices": [
      "Batch data only",
      "Streaming data only",
      "Both batch and streaming data",
      "Neither batch nor streaming data"
    ],
    "correct": "Both batch and streaming data",
    "description": "Spark supports batch via RDD/DataFrames and streaming via Structured Streaming."
  },
  {
      "id": 17,
      "question": "What is a key advantage of Spark over MapReduce?",
      "choices": [
        "Spark eliminates the need for clusters",
        "Spark provides in-memory computation for faster performance",
        "Spark can only process small datasets",
        "Spark does not support distributed processing"
      ],
      "correct": "Spark provides in-memory computation for faster performance",
      "description": "MapReduce writes intermediate results to disk; Spark keeps them in memory."
  },
  {
    "id": 18,
    "question": "Spark was donated to which foundation?",
    "choices": [
      "Apache Software Foundation",
      "Linux Foundation",
      "Eclipse Foundation",
      "Mozilla Foundation"
    ],
    "correct": "Apache Software Foundation",
    "description": "Spark became an Apache top-level project in 2014."
  },
  {
    "id": 19,
    "question": "What does the Spark driver do?",
    "choices": [
      "Executes tasks directly on all nodes",
      "Schedules tasks and manages application lifecycle",
      "Stores final results permanently",
      "Provides graphical dashboards"
    ],
    "correct": "Schedules tasks and manages application lifecycle",
    "description": "Driver is the central coordinator of a Spark application."
  },
  {
    "id": 20,
    "question": "Executors in Spark are responsible for:",
    "choices": [
      "Sending UI updates",
      "Running tasks and storing data",
      "Managing cluster nodes",
      "Installing libraries"
    ],
    "correct": "Running tasks and storing data",
    "description": "Executors perform distributed computations on worker nodes."
  },
  {
    "id": 21,
    "question": "Which engines can Spark use for file storage?",
    "choices": [
      "Only HDFS",
      "Only local disk",
      "Multiple storage systems like HDFS, S3, and local FS",
      "Only relational databases"
    ],
    "correct": "Multiple storage systems like HDFS, S3, and local FS",
    "description": "Spark supports a variety of storage layers through Hadoop-compatible connectors."
  },
  {
    "id": 22,
    "question": "Which programming environment provides an interactive way to work with Spark?",
    "choices": [
      "Spark Shell",
      "Adobe Photoshop",
      "Oracle SQL Developer",
      "Microsoft Visual Studio"
    ],
    "correct": "Spark Shell",
    "description": "Spark provides Scala and Python shells for interactive analytics."
  },
  {
    "id": 23,
    "question": "Which type of workloads benefit most from Spark’s in-memory design?",
    "choices": [
      "Single-pass batch processing",
      "Iterative machine learning and graph computations",
      "File indexing only",
      "Small local CSV analytics"
    ],
    "correct": "Iterative machine learning and graph computations",
    "description": "These require repeated access to the same dataset, making memory caching beneficial."
  },
  {
    "id": 24,
    "question": "Spark supports which data formats?",
    "choices": [
      "Parquet, Avro, JSON, CSV",
      "Only Parquet",
      "Only proprietary formats",
      "JPEG and MP3 only"
    ],
    "correct": "Parquet, Avro, JSON, CSV",
    "description": "Spark works with multiple structured and semi-structured data formats."
  },
  {
    "id": 25,
    "question": "What does the Spark UI primarily help with?",
    "choices": [
      "Creating dashboards",
      "Monitoring jobs, stages, and tasks",
      "Database administration",
      "GPU programming"
    ],
    "correct": "Monitoring jobs, stages, and tasks",
    "description": "Spark UI provides insight into performance and execution details."
  },
  {
    "id": 26,
    "question": "Spark is highly scalable because it uses:",
    "choices": [
      "Single-node execution only",
      "Distributed clusters of machines",
      "Dedicated GPU cores only",
      "Manual resource scheduling"
    ],
    "correct": "Distributed clusters of machines",
    "description": "Spark scales horizontally by distributing computation across many worker nodes."
  },
  {
    "id": 27,
    "question": "Which execution engine does Spark replace from Hadoop?",
    "choices": [
      "HDFS",
      "MapReduce",
      "HBase",
      "Pig"
    ],
    "correct": "MapReduce",
    "description": "Spark offers a faster alternative to MapReduce for data processing."
  },
  {
    "id": 28,
    "question": "Which type of API does Spark SQL introduce?",
    "choices": [
      "Unstructured RDD-only API",
      "Structured API with DataFrames",
      "Low-level binary coding API",
      "Mobile UI API"
    ],
    "correct": "Structured API with DataFrames",
    "description": "DataFrames provide schema-based structured data operations."
  },
  {
    "id": 29,
    "question": "Spark’s fault-tolerance is supported by:",
    "choices": [
      "Data replication only",
      "RDD lineage and recomputation",
      "UI alerts",
      "Manual code rewrites"
    ],
    "correct": "RDD lineage and recomputation",
    "description": "Lineage tracks transformations and recomputes lost data on failure."
  },
  {
    "id": 30,
    "question": "Spark is most suitable for:",
    "choices": [
      "Distributed analytics workloads",
      "Small Excel tasks",
      "Video editing",
      "Game development"
    ],
    "correct": "Distributed analytics workloads",
    "description": "Spark is designed for large-scale data engineering and analytics."
  },
  {
    "id": 31,
    "question": "What does Apache Spark primarily optimize for?",
    "choices": [
      "Memory and execution speed",
      "Low power consumption",
      "3D rendering performance",
      "Mobile compatibility"
    ],
    "correct": "Memory and execution speed",
    "description": "Spark aims to process data faster by using memory-centric execution."
  },
  {
    "id": 32,
    "question": "Structured Streaming in Spark is built on top of:",
    "choices": [
      "DataFrames",
      "RDD transformations only",
      "Kafka APIs",
      "GraphX"
    ],
    "correct": "DataFrames",
    "description": "Structured Streaming uses the same engine as DataFrames and Spark SQL."
  },
  {
    "id": 33,
    "question": "Which of the following best describes Spark’s ecosystem?",
    "choices": [
      "A relational database product",
      "A unified analytics engine",
      "A file compression standard",
      "A single-threaded processing tool"
    ],
    "correct": "A unified analytics engine",
    "description": "Spark unifies batch, streaming, SQL, ML & graphs under one platform."
  },
  {
    "id": 34,
    "question": "Which two operations form the core of Spark’s execution model?",
    "choices": [
      "Transformations and Actions",
      "Queries and Dashboards",
      "Insert and Update",
      "Encode and Decode"
    ],
    "correct": "Transformations and Actions",
    "description": "Transformations define lineage; actions trigger execution in Spark."
  }
]
